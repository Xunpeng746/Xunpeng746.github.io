---
---

@string{aps = {American Physical Society,}}

@article{zhang2018finding,
  abbr={Inf. Sci},
  bibtex_show={true},
  title={Finding potential lenders in P2P lending: A hybrid random walk approach},
  author={Zhang, Hefu and Zhao, Hongke and Liu, Qi and Xu, Tong and Chen, Enhong and Huang, Xunpeng},
  journal={Information Sciences},
  volume={432},
  pages={376--391},
  year={2018},
  publisher={Elsevier}
}

@inproceedings{wang2018confidence,
  abbr={AAAI},
  bibtex_show={true},
  title={Confidence-aware matrix factorization for recommender systems},
  author={Wang, Chao and Liu, Qi and Wu, Runze and Chen, Enhong and Liu, Chuanren and Huang, Xunpeng and Huang, Zhenya},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={32},
  number={1},
  year={2018}
}

@inproceedings{huang2017incremental,
  abbr={IJCAI},
  bibtex_show={true},
  title={Incremental Matrix Factorization: A Linear Feature Transformation Perspective.},
  author={Huang, Xunpeng and Wu, Le and Chen, Enhong and Zhu, Hengshu and Liu, Qi and Wang, Yijun},
  abstract={Matrix Factorization (MF) is among the most widely used techniques for collaborative filtering based recommendation. Along this line, a critical demand is to incrementally refine the MF models when new ratings come in an online scenario. However, most of existing incremental MF algorithms are limited by specific MF models or strict use restrictions. In this paper, we propose a general incremental MF framework by designing a linear transformation of user and item latent vectors over time. This framework shows a relatively high accuracy with a computation and space efficient training process in an online scenario. Meanwhile, we explain the framework with a low-rank approximation perspective, and give an upper bound on the training error when this framework is used for incremental learning in some special cases. Finally, extensive experimental results on two real-world datasets clearly validate the effectiveness, efficiency and storage performance of the proposed framework.},
  booktitle={IJCAI},
  pages={1901--1908},
  year={2017},
  html={https://www.ijcai.org/proceedings/2017/0264.pdf},
  selected={true}
}

@inproceedings{guo2018enhancing,
  abbr={DASFAA},
  bibtex_show={true},
  title={Enhancing network embedding with auxiliary information: An explicit matrix factorization perspective},
  author={Guo, Junliang and Xu, Linli and Huang, Xunpeng and Chen, Enhong},
  booktitle={International Conference on Database Systems for Advanced Applications},
  pages={3--19},
  year={2018},
  organization={Springer}
}

@inproceedings{huang2020span,
  abbr={AAAI},
  bibtex_show={true},
  title={Span: A stochastic projected approximate newton method},
  author={Huang, Xunpeng and Liang, Xianfeng and Liu, Zhengyang and Li, Lei and Yu, Yue and Li, Yitan},
  abstract={Second-order optimization methods have desirable convergence properties. However, the exact Newton method requires expensive computation for the Hessian and its inverse. In this paper, we propose SPAN, a novel approximate and fast Newton method. SPAN computes the inverse of the Hessian matrix via low-rank approximation and stochastic Hessian-vector products. Our experiments on multiple benchmark datasets demonstrate that SPAN outperforms existing first-order and second-order optimization methods in terms of the convergence wall-clock time. Furthermore, we provide a theoretical analysis of the per-iteration complexity, the approximation error, and the convergence rate. Both the theoretical analysis and experimental results show that our proposed method achieves a better trade-off between the convergence rate and the per-iteration efficiency.},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={34},
  number={02},
  pages={1520--1527},
  year={2020},
  html={https://ojs.aaai.org/index.php/AAAI/article/view/5511},
  selected={true}
}

@inproceedings{huang2021acmo,
  abbr={AAAI},
  bibtex_show={true},
  title={ACMo: Angle-Calibrated Moment Methods for Stochastic Optimization},
  author={Huang, Xunpeng and Xu, Runxin and Zhou, Hao and Wang, Zhe and Liu, Zhengyang and Li, Lei},
  abstract={Stochastic gradient descent (SGD) is a widely used method for its outstanding generalization ability and simplicity. Adaptive gradient methods have been proposed to further accelerate the optimization process. In this paper, we revisit existing adaptive gradient optimization methods with a new interpretation. Such new perspective leads to a refreshed understanding of the roles of second moments in stochastic optimization. Based on this, we propose Angle-Calibration Moment method (ACMo), a novel stochastic optimization method. It enjoys the benefits of second moments with only first moment updates. Theoretical analysis shows that ACMo is able to achieve the same convergence rate as mainstream adaptive methods. Experiments on a variety of CV and NLP tasks demonstrate that ACMo has a comparable convergence to state-of-the-art Adam-type optimizers, and even a better generalization performance in most cases. The code is available at https://github.com/Xunpeng746/ACMo.},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={35},
  number={9},
  pages={7857--7864},
  year={2021},
  html={https://ojs.aaai.org/index.php/AAAI/article/view/16959},
  selected={true}
}
